{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMxa3Zw4yNmuigYrQh5emZt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## Libraries & Setting & Utils"],"metadata":{"id":"tNny-htVp2lC"}},{"cell_type":"code","source":["! pip install timm\n","! pip install ttach\n","! pip install pytorch_lightning"],"metadata":{"id":"AtRp7a7Bp5Xs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668044750995,"user_tz":-540,"elapsed":27883,"user":{"displayName":"yeji Kim","userId":"05242765081362632827"}},"outputId":"b1626591-8672-44c6-ba1b-755e861dfa5a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n","\u001b[K     |████████████████████████████████| 548 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n","Collecting huggingface-hub\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 72.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from timm) (6.0)\n","Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->timm) (4.1.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (3.8.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.13.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->timm) (3.10.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2022.9.24)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Installing collected packages: huggingface-hub, timm\n","Successfully installed huggingface-hub-0.10.1 timm-0.6.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ttach\n","  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n","Installing collected packages: ttach\n","Successfully installed ttach-0.0.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_lightning\n","  Downloading pytorch_lightning-1.8.0.post1-py3-none-any.whl (796 kB)\n","\u001b[K     |████████████████████████████████| 796 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.10.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.6)\n","Collecting torchmetrics>=0.7.0\n","  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n","\u001b[K     |████████████████████████████████| 529 kB 48.1 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.1.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n","Collecting lightning-utilities==0.3.*\n","  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n","Collecting lightning-lite==1.8.0.post1\n","  Downloading lightning_lite-1.8.0.post1-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.9.1)\n","Collecting fire\n","  Downloading fire-0.4.0.tar.gz (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.23.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (0.13.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.38.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.35.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.4.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.17.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (57.4.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.6.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.50.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (4.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (3.10.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (3.2.2)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->lightning-utilities==0.3.*->pytorch_lightning) (2.1.0)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=4f4b1291bdd5da68cf77b89f1b1217662f1c6317af4362d6709640968500c593\n","  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n","Successfully built fire\n","Installing collected packages: fire, lightning-utilities, torchmetrics, lightning-lite, pytorch-lightning\n","Successfully installed fire-0.4.0 lightning-lite-1.8.0.post1 lightning-utilities-0.3.0 pytorch-lightning-1.8.0.post1 torchmetrics-0.10.2\n"]}]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import transforms\n","from torchvision.datasets import Flowers102, ImageNet\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","import timm\n","import urllib\n","from PIL import Image\n","\n","import time\n","import json\n","import torch\n","import logging\n","import argparse\n","from collections import OrderedDict\n","import sys, os\n","\n","from timm.utils import accuracy, AverageMeter, setup_default_logging\n","\n","import ttach as tta\n","\n","from torch.utils.data import random_split\n","import pandas as pd\n","\n","from pandas.io.formats.info import DataFrameTableBuilderVerbose\n","import os\n","import pandas as pd\n","from torchvision.io import read_image\n","from torchvision.transforms.functional import to_pil_image\n","from torchvision.transforms import ToTensor\n","\n","from torch.autograd import grad\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from torch import optim \n","\n","import matplotlib.pyplot as plt \n","from torchvision.models.mobilenetv2 import MobileNetV2"],"metadata":{"id":"MP0PHYEcp6ve","executionInfo":{"status":"ok","timestamp":1668045133805,"user_tz":-540,"elapsed":5264,"user":{"displayName":"yeji Kim","userId":"05242765081362632827"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","else:\n","  device = torch.deivce('cpu')\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"I6K8QE3Tp-j_","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"error","timestamp":1668045140645,"user_tz":-540,"elapsed":4,"user":{"displayName":"yeji Kim","userId":"05242765081362632827"}},"outputId":"ef096e27-1fd2-4aaa-e45f-52e317578b76"},"execution_count":3,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6705402bf0aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeivce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'deivce'"]}]},{"cell_type":"code","source":["# weight initialization\n","def initialize_weights(model):\n","    classname = model.__class__.__name__\n","    # fc layer\n","    if classname.find('Linear') != -1:\n","        nn.init.normal_(model.weight.data, 0.0, 0.02)\n","        nn.init.constant_(model.bias.data, 0)\n","    # batchnorm\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(model.weight.data, 1.0, 0.02)\n","        nn.init.constant_(model.bias.data, 0)\n","    \n","    elif classname.find('Conv') != -1:\n","        nn.init.normal_(model.weight.data, 1.0, 0.02)\n","        nn.init.constant_(model.bias.data, 0)"],"metadata":{"id":"9koV5YnkqFtS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_grad_flow(named_parameters):\n","    ave_grads = []\n","    layers = []\n","    for n, p in named_parameters:\n","      if(p.requires_grad) and (\"bias\" not in n):\n","        if p.grad != None:\n","          layers.append(n)\n","          ave_grads.append(p.grad.abs().mean().cpu().numpy())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n","    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient flow\")\n","    plt.grid(True)"],"metadata":{"id":"vCgFumT6qHf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activation = {}\n","def get_activation(name):\n","    def hook(model, input, output):\n","        activation[name] = output.detach()\n","    return hook\n","\n","def check_acc(model, test_dl, ckpt_path):\n","  correct = 0\n","  total = 0\n","\n","  checkpoint=torch.load(ckpt_path)\n","  m=model\n","  m.load_state_dict(checkpoint['state_dict'], strict=False)\n","  m=m.to(device)\n","  with torch.no_grad():\n","    for batch in test_dl:\n","      images, labels = batch\n","      images, labels = images.to(device), labels.to(device)\n","      outputs = m(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","      #print((predicted == labels).sum().item()/labels.size(0))\n","\n","  return 100 * correct / float(total)"],"metadata":{"id":"TbguWnbfqJMt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"djfNZHAbqR4t"}},{"cell_type":"markdown","source":["1. Flower 102"],"metadata":{"id":"Uk7U64zmqYV_"}},{"cell_type":"code","source":["image_preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = Flowers102(root = '.', split = \"train\", transform = image_preprocess, download=True)\n","validation_dataset = Flowers102(root = '.', split = \"val\", transform = image_preprocess)\n","test_dataset = Flowers102(root = '.', split = \"test\", transform = image_preprocess)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n","validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False, num_workers=2)\n","test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2, drop_last=True)"],"metadata":{"id":"ydQrRcz1qUF0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Augmented Flower 102"],"metadata":{"id":"2A19Ktn7qc0M"}},{"cell_type":"code","source":["image_preprocess_hf = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.RandomHorizontalFlip()\n","])\n","\n","image_preprocess_vf = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.RandomVerticalFlip()\n","])\n","\n","image_preprocess_rr = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.RandomRotation(90)\n","])\n","\n","\n","\n","train_dataset_hf = Flowers102(root = '.', split = \"train\", transform = image_preprocess_hf, download=True)\n","validation_dataset_hf = Flowers102(root = '.', split = \"val\", transform = image_preprocess_hf)\n","test_dataset_hf = Flowers102(root = '.', split = \"test\", transform = image_preprocess_hf)\n","\n","train_dataset_vf = Flowers102(root = '.', split = \"train\", transform = image_preprocess_vf, download=True)\n","validation_dataset_vf = Flowers102(root = '.', split = \"val\", transform = image_preprocess_vf)\n","test_dataset_vf = Flowers102(root = '.', split = \"test\", transform = image_preprocess_vf)\n","\n","train_dataset_rr = Flowers102(root = '.', split = \"train\", transform = image_preprocess_rr, download=True)\n","validation_dataset_rr = Flowers102(root = '.', split = \"val\", transform = image_preprocess_rr)\n","test_dataset_rr = Flowers102(root = '.', split = \"test\", transform = image_preprocess_rr)\n","\n","train_f = torch.utils.data.ConcatDataset([train_dataset, train_dataset_hf, \n","                                          train_dataset_vf, train_dataset_rr])\n","validation_f = torch.utils.data.ConcatDataset([validation_dataset, validation_dataset_hf, \n","                                          validation_dataset_vf, validation_dataset_rr])\n","test_f = torch.utils.data.ConcatDataset([test_dataset, test_dataset_hf, \n","                                          test_dataset_vf, test_dataset_rr])\n","\n","train_dataloader_af = DataLoader(train_f, batch_size=8, shuffle=True, num_workers=2)\n","validation_dataloader_af = DataLoader(validation_f, batch_size=1, shuffle=False, num_workers=2)\n","test_dataloader_af = DataLoader(test_f, batch_size=8, shuffle=False, num_workers=2, drop_last=True)"],"metadata":{"id":"lmL2AblXqaGO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. ImageNet Validation Set"],"metadata":{"id":"p0zmGXRjqh3H"}},{"cell_type":"code","source":["t=pd.read_csv('/content/drive/Shareddrives/Data/ILSVRC2012_img_val.tar (Unzipped Files)/ILSVRC2012_validation_ground_truth.txt', sep=\"\\n\", header=None)\n","arr=['ILSVRC2012_val_'+str(i).zfill(8)+'.JPEG' for i in range(1, 50001)]\n","df = pd.DataFrame({'col':arr})\n","df['label']=t-1\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, img_dir, transform=None, target_transform=None):\n","        self.img_labels = df\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = Image.open(img_path).convert('RGB')\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label\n","\n","\n","path='/content/drive/Shareddrives/Data/ILSVRC2012_img_val.tar (Unzipped Files)'\n","imagenet_train_dataset, imagenet_validation_dataset=random_split(CustomImageDataset(path, transform = image_preprocess), [40000, 10000])\n","\n","imagenet_train_dataloader = DataLoader(imagenet_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n","imagenet_validation_dataloader = DataLoader(imagenet_validation_dataset, batch_size=32, shuffle=False, num_workers=0)"],"metadata":{"id":"s298Dhfsqlwl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Models"],"metadata":{"id":"-m2NS1NEqqW8"}},{"cell_type":"markdown","source":["1-1. ViT"],"metadata":{"id":"YspYzaz0q-GZ"}},{"cell_type":"code","source":["model = timm.create_model('vit_tiny_r_s16_p8_224', pretrained=True)\n","model.eval()\n","model.to(device)\n","\n","image_preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor()\n","])"],"metadata":{"id":"htJRFQlaqoHJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1-2. ViT + additional layers"],"metadata":{"id":"UZXj5kRjrM6l"}},{"cell_type":"code","source":["class vitp(nn.Module):\n","    def __init__(self):\n","        super(vitp, self).__init__()\n","        self.vit = timm.create_model('vit_tiny_r_s16_p8_224', pretrained=True).to(device)\n","        self.vit.head=nn.Identity()\n","        #self.linear=nn.Linear(1536, 1728)\n","        self.mob=MobileNetV2(num_classes=102).to(device)\n","        for param in self.vit.parameters():\n","          param.requires_grad=False\n","\n","    def forward(self, x):\n","        x=self.vit(x)\n","        #x=self.linear(x)\n","        print(x.shape)\n","        x=torch.reshape(x, [-1, 3, 8, 8])\n","        logits = self.mob(x)\n","        #logits=nn.Softmax(logits)\n","        return logits"],"metadata":{"id":"PoqSI5lLrK5I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2-1. NAT-Flower 102"],"metadata":{"id":"Gd60L4cIrW_G"}},{"cell_type":"code","source":["sys.path.append('/content/drive/MyDrive/neural-architecture-transfer-master')\n","from codebase.networks.natnet import NATNet\n","from codebase.modules.layers import set_layer_from_config\n","from codebase.data_providers.factory import get_dataloader\n","\n","net_config = json.load(open('/content/drive/MyDrive/neural-architecture-transfer-master/subnets/flowers102/net-img@240-flops@400-top1@98.3/net.config'))\n","student = NATNet.build_from_config(net_config, pretrained=None)"],"metadata":{"id":"d-gotdYWrZV4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2-2. NAT-Flower 102 + additional layers"],"metadata":{"id":"t4WEYKJHrjIL"}},{"cell_type":"code","source":["class finalModel(nn.Module):\n","    def __init__(self, train_partially=True):\n","        super(finalModel, self).__init__()\n","        self.pretrained=student\n","        checkpoint=torch.load('/content/drive/Shareddrives/Data/deepest_model/0914-KD-ImageNet-epoch=09-loss=2.75.ckpt')\n","        self.pretrained.load_state_dict(checkpoint['state_dict'], strict=False)\n","        self.additional = nn.Sequential(\n","            nn.Linear(1536, 512),\n","            nn.Mish(),\n","            nn.Linear(512, 102)\n","        )\n","        if train_partially: \n","          for param in self.pretrained.parameters():\n","            param.requires_grad=False\n","          for param in self.additional.parameters():\n","            param.requires_grad=True\n","          \n","          \n","        else:\n","          for param in self.pretrained.parameters():\n","            param.requires_grad=True\n","          for param in self.additional.parameters():\n","            param.requires_grad=True\n","        \n","\n","    def forward(self, x):\n","        x=self.pretrained(x)\n","        logits = self.additional(x)\n","        return logits"],"metadata":{"id":"diS1XEhrrmH3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"HKTOcWAurrlU"}},{"cell_type":"markdown","source":["1-1. Knowledge Distillation on ImageNet dataset"],"metadata":{"id":"2prcvglGrt6S"}},{"cell_type":"code","source":["def dist(y, labels, teacher_scores, T=20.0, alpha=0.7):\n","    # distillation loss + classification loss\n","    # y: student\n","    # labels: hard label\n","    # teacher_scores: soft label\n","    return nn.KLDivLoss(reduction=\"batchmean\")(F.log_softmax(y/T, -1), F.softmax(teacher_scores/T, -1)) * (T*T * 2.0 + alpha) + F.cross_entropy(y,labels) * (1.-alpha)\n","\n","\n","class kd(pl.LightningModule):\n","  def __init__(self):\n","    super(kd, self).__init__()\n","    self._student_model=student.to(device)\n","    self._teacher_model=model.to(device)\n","    for param in self._teacher_model.parameters():\n","      param.requires_grad=False\n","    self._tdl=imagenet_train_dataloader\n","    self.vdl=imagenet_validation_dataloader\n","  \n","  def forward(self, images):\n","    return self._student_model(images)\n","\n","  def training_step(self, batch):\n","    images, labels=[i.float() for i in batch]\n","    images, labels=images.to(device), labels.to(device)\n","    \n","    self._student_model=self._student_model.to(device)\n","    self._teacher_model=self._teacher_model.to(device)\n","    \n","    y_hat_student=self.forward(images)\n","    y_hat_teacher=self._teacher_model(images)\n","\n","    loss=dist(y_hat_student, labels.long(), y_hat_teacher)\n","    print(loss)\n","    self.log(\"loss\", loss)\n","    return {'loss':loss}\n","\n","  def train_dataloader(self):\n","    return self._tdl\n","  \n","  def validation_dataloader(self):\n","    return self._vdl\n","\n","  def configure_optimizers(self):\n","    optimizer=optim.RMSprop(self._student_model.parameters(), lr=0.01)\n","    scheduler=optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=0.001)\n","    return [optimizer], [scheduler]\n","\n","  def validation_step(self, batch):\n","    images, labels=[i.float() for i in batch]\n","    images, labels=images.to(device), labels.to(device)\n","\n","    y_hat=self.forward(images)\n","    return {'loss': nn.CrossEntropyLoss()(y_hat, labels.long())}\n"],"metadata":{"id":"EiAdtca_rx24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(tt_dl=imagenet_train_dataloader, vv_dl=imagenet_validation_dataloader):\n","  checkpoint_callback = ModelCheckpoint(dirpath=\"/content/drive/Shareddrives/Data/deepest_model\", \n","                                        save_top_k=5, monitor=\"loss\", mode=\"min\",\n","                                        filename=\"0914-KD-ImageNet-{epoch:02d}-{loss:.2f}\")\n","  #training_module=DistilledTrainingModule({'learning_rate':0.001, 'momentum':0.9}, t_dl=tt_dl, v_dl=vv_dl)\n","  training_module=kd()\n","  \n","  trainer=pl.Trainer(default_root_dir=\"/content/drive/Shareddrives/Data/deepest_model\", callbacks=[checkpoint_callback])\n","  \n","  trainer.fit(training_module)"],"metadata":{"id":"PuucJ9gBsjZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["initialize_weights(student)\n","train()"],"metadata":{"id":"KpdFtMaoslGk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1-2. Transfer learning"],"metadata":{"id":"Ba8KklAjso0l"}},{"cell_type":"code","source":["student.classifier=nn.Identity()"],"metadata":{"id":"n7PX7TXWsq4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class train_f(pl.LightningModule):\n","  def __init__(self, tp, pt=''):\n","    super().__init__()\n","    self._model=finalModel(train_partially=tp).to(device)\n","  \n","    self._tdl=train_dataloader\n","    self._vdl=validation_dataloader\n","    self._loss=nn.CrossEntropyLoss()\n","\n","  def forward(self, images):\n","    self._model=self._model.to(device)\n","    return self._model(images)\n","  \n","  def training_step(self, batch):\n","    images, labels=[i.float() for i in batch]\n","    images, labels=images.to(device), labels.to(device)\n","\n","    y_hat=self.forward(images)\n","    loss=self._loss(y_hat, labels.long())\n","    #print(loss)\n","    self.log(\"loss\", loss)\n","    plot_grad_flow(self._model.named_parameters())\n","    return {'loss':loss}\n","\n","  def train_dataloader(self):\n","    return self._tdl\n","\n","  def validation_dataloader(self):\n","    return self._vdl\n","\n","  def configure_optimizers(self):\n","    optimizer=optim.RMSprop(self._model.parameters(), lr=0.001)\n","    scheduler=optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=0.001)\n","    return [optimizer], [scheduler]\n","  \n","  def validation_step(self, batch):\n","    images, labels=[i.float() for i in batch]\n","    images, labels=images.to(device), labels.to(device)\n","\n","    y_hat=self.forward(images)\n","    return {'loss': self._loss(y_hat, labels.long())}"],"metadata":{"id":"NsNz6Nn7s4DW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_lightning import loggers as pl_loggers\n","from pytorch_lightning.callbacks import EarlyStopping\n","\n","def train2(train_partially, ptweight='', resume=False):\n","  early_stopping = EarlyStopping('loss')\n","  checkpoint_callback = ModelCheckpoint(dirpath=\"/content/drive/Shareddrives/Data/deepest_model\", \n","                                        save_top_k=5, monitor=\"loss\", mode=\"min\",\n","                                        filename=\"0915TL-F102+AUG-{epoch:02d}-{loss:.2f}\")\n","  #training_module=DistilledTrainingModule({'learning_rate':0.001, 'momentum':0.9}, t_dl=tt_dl, v_dl=vv_dl)\n","  training_module=train_f(train_partially)\n","\n","  checkpoint=torch.load(ptweight)\n","  training_module.load_state_dict(checkpoint['state_dict'], strict=False)\n","\n","  tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"/content/drive/Shareddrives/Data/deepest_model/log\")\n","  if resume:\n","    trainer=pl.Trainer(default_root_dir=\"/content/drive/Shareddrives/Data/deepest_model\", \n","                       callbacks=[checkpoint_callback, early_stopping], logger=tb_logger,\n","                       resume_from_checkpoint=checkpoint)\n","  else:\n","    trainer=pl.Trainer(default_root_dir=\"/content/drive/Shareddrives/Data/deepest_model\", \n","                       callbacks=[checkpoint_callback, early_stopping], logger=tb_logger)\n","  \n","  trainer.fit(training_module)"],"metadata":{"id":"Unfv3Yvms-gn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train2(False, ptweight='/content/drive/Shareddrives/Data/deepest_model/0915TL-F102+AUG-epoch=12-loss=3.67.ckpt')"],"metadata":{"id":"FzRzfELptByx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2-1. Knowledge Distillation"],"metadata":{"id":"LSarLJaHtDvR"}},{"cell_type":"code","source":["val_acc_epoch=[]\n","val_loss_step=[]\n","\n","class train_vitp(pl.LightningModule):\n","  def __init__(self):\n","    super().__init__()\n","    self._model=vitp().to(device)\n","  \n","    self._tdl=train_dataloader_af\n","    self._vdl=validation_dataloader_af\n","    self._loss=nn.CrossEntropyLoss()\n","\n","  def forward(self, images):\n","    self._model=self._model.to(device)\n","    return self._model(images)\n","  \n","  def training_step(self, batch):\n","    images, labels=[i.float() for i in batch]\n","    images, labels=images.to(device), labels.to(device)\n","\n","    y_hat=self.forward(images)\n","    loss=self._loss(y_hat, labels.long())\n","    self.log(\"loss\", loss)\n","    self.logger.experiment.add_scalar(\"Train loss\", loss, self.global_step)\n","\n","    return {'log':{'loss':loss}, 'loss':loss}\n","\n","  def train_dataloader(self):\n","    return self._tdl\n","\n","  def validation_dataloader(self):\n","    return self._vdl\n","\n","  def configure_optimizers(self):\n","    optimizer=optim.Adam(self._model.parameters(), lr=0.001)\n","    scheduler=optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=0.001)\n","    return [optimizer], [scheduler]\n","  \n","  def validation_step(self, batch):\n","    images, labels=[i.float() for i in batch]\n","    images, labels=images.to(device), labels.to(device)\n","\n","    y_hat=self.forward(images)\n","    loss=self._loss(y_hat, labels.long())\n","    self.log('val_loss', loss)\n","    val_loss_step.append(loss)\n","    self.logger.experiment.add_scalar(\"Val loss\", loss, self.global_step)\n","      \n","    return {'log':{'val_loss': loss}, 'loss':loss}\n","\n","  def validation_epoch_end(self):\n","    with torch.no_grad():\n","      for batch in tqdm(test_dataloader):\n","        images, labels = batch\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = self.forward(images)\n","        _, predicted = torch.max(outputs.data, 0)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","      accuracy= 100 * correct / float(total)\n","      val_acc_epoch.append(accuracy)\n","      self.log(\"val_accuracy\", accuracy)\n","      self.logger.experiment.add_scalar(\"Val acc\", accuracy, self.current_epoch)\n","      return {'log':{'val_accuracy': accuracy}, 'accuracy':accuracy}"],"metadata":{"id":"b_TNOVJwtGf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(tt_dl=imagenet_train_dataloader, vv_dl=imagenet_validation_dataloader):\n","\n","  early_stopping = EarlyStopping('loss', patience=10)\n","  checkpoint_callback = ModelCheckpoint(dirpath=\"/content/drive/Shareddrives/Data/deepest_model\", \n","                                        save_top_k=5, monitor=\"loss\", mode=\"min\",\n","                                        filename=\"0916KD-F102-{epoch:02d}-{loss:.5f}\")\n","  #training_module=DistilledTrainingModule({'learning_rate':0.001, 'momentum':0.9}, t_dl=tt_dl, v_dl=vv_dl)\n","  training_module=kd('/content/drive/Shareddrives/Data/deepest_model/0915vitp-F102+AUG5-epoch=13-loss=0.00.ckpt')\n","\n","  #training_module.load_state_dict(checkpoint['state_dict'], strict=False)\n","  tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"/content/drive/Shareddrives/Data/deepest_model/log_kd\")\n","  trainer=pl.Trainer(default_root_dir=\"/content/drive/Shareddrives/Data/deepest_model\", \n","                       callbacks=[checkpoint_callback, early_stopping], logger=tb_logger)\n","  \n","  trainer.fit(training_module)"],"metadata":{"id":"SkyaGEnAtUtt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["initialize_weights(student)\n","train()"],"metadata":{"id":"1FHZzT19tek9"},"execution_count":null,"outputs":[]}]}